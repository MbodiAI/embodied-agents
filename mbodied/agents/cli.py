import json
import rich_click as click
from rich import print
from mbodied.agents.language import LanguageAgent
from mbodied.agents.sense.depth_estimation_agent import DepthEstimationAgent
from mbodied.agents.sense.object_detection_agent import ObjectDetectionAgent
from mbodied.agents.sense.segmentation_agent import SegmentationAgent
from mbodied.agents.auto.auto_agent import AutoAgent
from mbodied.agents.motion.openvla_agent import OpenVlaAgent
from mbodied.types.sense.vision import Image
from mbodied.types.sense.world import BBox2D, PixelCoords
from mbodied import __version__


@click.group()
@click.option("--verbose", "-v", is_flag=True, help="Enable verbose output.")
@click.option("--dry-run", is_flag=True, help="Simulate the action without executing.")
@click.pass_context
def cli(ctx, verbose, dry_run) -> None:
    """CLI for various AI agents."""
    ctx.ensure_object(dict)
    ctx.obj['VERBOSE'] = verbose
    ctx.obj['DRY_RUN'] = dry_run

    if verbose:
        click.echo("Verbose mode enabled.")
    if dry_run:
        click.echo("Dry run mode enabled.")

@cli.command("list")
def list_agents() -> None:
    """List all available agent types."""
    agent_types = [
        "OpenVLA Agent - Generate robot motion based on image and instructions.",
        "Auto Agent - Dynamically select and run the appropriate agent based on the task.",
        "Object Detection Agent - Detect objects in an image.",
        "Image Segmentation Agent - Segment objects in an image.",
        "Depth Estimation Agent - Estimate depth from an image.",
        "Language Agent - Interact with languge model using natural language."
    ]
    
    click.echo("Available Agent Types:")
    for agent in agent_types:
        click.echo(f"- {agent}")

@cli.command("language")
@click.option(
    "--model-src",
    default="openai",
    help="The model source for the LanguageAgent. i.e. openai, anthropic, gradio url, etc",
)
@click.option("--api-key", default=None, help="API key for the remote actor (if applicable).")
@click.option("--context", default=None, help="Starting context for the conversation.")
@click.option("--instruction", prompt="Instruction", help="Instruction for the LanguageAgent.")
@click.option("--image-path", default=None, help="Optional path to the image file.")
@click.option("--loop", is_flag=True, help="Keep the agent running for multiple instructions.")
@click.pass_context
def language_chat(ctx, model_src, api_key, context, instruction, image_path, loop) -> None:
    """
    Run the LanguageAgent to interact with users using natural language.

    Example command:
        mbodied language --instruction "What type of robot is this?" --image-path resources/color_image.png

    Response:
        This is a robotic arm, specifically a PR2 (Personal Robot 2) developed by Willow Garage.
    
    Inputs:
        [model_src]: The model source for the LanguageAgent (e.g., openai, anthropic, or a gradio URL).
        [api_key]: Optional API key for the remote actor, if needed.
        [context]: Starting context for the conversation (optional).
        [instruction]: Instruction or query for the LanguageAgent to respond to.
        [image_path]: Optional path to an image file to include as part of the input.
        [loop]: If set, the agent will continue running and accepting new instructions.

    Outputs:
        [Response]: The natural language response generated by the LanguageAgent.
    """
    verbose = ctx.obj['VERBOSE']
    dry_run = ctx.obj['DRY_RUN']

    if verbose:
        click.echo(f"Running language agent from {model_src}")

    if dry_run:
        click.echo(f"Dry run: Would run LanguageAgent with model: {model_src}, instruction: {instruction}")
        return

    agent = LanguageAgent(model_src=model_src, api_key=api_key, context=context)
    image = Image(image_path) if image_path else None
    while True:
        response = agent.act(instruction=instruction, image=image, context=context)
        print("Response:", response)
        
        if not loop:
            break
        
        instruction = input("Enter new instruction (or 'exit-loop' to stop): ")
        if instruction.lower() == "exit-loop":
            break

@cli.group(invoke_without_command=True)
@click.option("--list", "-l", is_flag=True, help="List available sensory models.")
@click.pass_context
def sense(ctx, list):
    """Commands related to sensing tasks (detection, segmentation, depth estimation)."""
    if list:
        click.echo("Available Sensory Models:")
        click.echo("- Object Detection Models:")
        click.echo("  - Grounding DINO")
        click.echo("  - YOLOWorld")
        click.echo("- Depth Estimation Models:")
        click.echo("  - Depth Anything")
        click.echo("  - Zoe Depth")
        click.echo("- Segmentation Models:")
        click.echo("  - Segment Anything(SAM2)")
        ctx.exit()

    if ctx.invoked_subcommand is None:
            click.echo("No subcommand provided. Try 'mbodied sense --help' for help")

@sense.command("detect")
@click.argument("image_filename", required=False)
@click.option("--model-src", default="https://api.mbodi.ai/sense/", help="The model source URL.")
@click.option(
    "--objects", prompt=False, help="Comma-separated list of objects to detect."
)
@click.option(
    "--model-type",
    type=click.Choice(["YOLOWorld", "Grounding DINO"], case_sensitive=False),
    prompt=False,
    help="The model type to use for detection.",
)
@click.option("--api-name", default="/detect", help="The API endpoint to use.")
@click.option("--list", "-l", is_flag=True, help="List available models for object detection.")
@click.pass_context
def detect_objects(ctx, image_filename, model_src, objects, model_type, api_name, list) -> None:
    """
    Run the ObjectDetectionAgent to detect objects in an image.

    Example command:
        mbodied sense detect resources/color_image.png --objects "remote, spoon" --model-type "YOLOWorld"

    Response:
        Annotated Image: The image with detected objects highlighted and labeled.
    
    Inputs:
        [image_filename]: Path to the image file.
        [objects]: Comma-separated list of objects to detect (e.g., "car, person").
        [model_type]: Model type to use for detection (e.g., "YOLOWorld", "Grounding DINO").

    Outputs:
        [Annotated Image]: Display of the image with detected objects and their bounding boxes.

    API documentation: https://api.mbodi.ai/sense/
    """
    verbose = ctx.obj['VERBOSE']
    dry_run = ctx.obj['DRY_RUN']

    if list:
        click.echo("Available Object Detection Models:")
        click.echo("- Grounding DINO")
        click.echo("- YOLOWorld")
        return
    
    if image_filename is None:
        click.echo("Error: Missing argument 'IMAGE_FILENAME'. Specify an image filename")
        return
    
    if objects is None:
        objects = click.prompt(
            "Objects to detect (comma-separated)"
        )
    
    if model_type is None:
        model_type = click.prompt(
            "Model Type", 
            type=click.Choice(["YOLOWorld", "Grounding DINO"], case_sensitive=False)
        )

    if verbose:
        click.echo(f"Running object detection on {image_filename} using {model_type}")

    if dry_run:
        click.echo(f"Dry run: Would detect objects in {image_filename} with model: {model_type}, objects: {objects}")
        return

    image = Image(image_filename, size=(224, 224))
    objects_list = objects.split(",")
    agent = ObjectDetectionAgent(model_src=model_src)
    result = agent.act(image=image, objects=objects_list, model_type=model_type, api_name=api_name)
    if verbose:
        click.echo("Displaying annotated image.")
    result.annotated.pil.show()

@sense.command("depth")
@click.argument("image_filename", required=False)
@click.option("--model-src", default="https://api.mbodi.ai/sense/", help="The model source URL.")
@click.option("--api-name", default="/depth", help="The API endpoint to use.")
@click.option("--list", "-l", is_flag=True, help="List available models for depth estimation.")
@click.pass_context
def estimate_depth(ctx, image_filename, model_src, api_name, list) -> None:
    """Run the DepthEstimationAgent to estimate depth from an image.

    Example command:
        mbodied sense depth path/to/image.png

    Response:
        Depth map image displaying the estimated depth information for each pixel.

    Inputs:
        [image_filename]: Path to the image file (e.g., PNG or RGBD image).

    Outputs:
        [Depth Estimation Response]: A depth map image representing the depth information in the image.
    
    Loaded as API: [https://api.mbodi.ai/sense/depth](https://api.mbodi.ai/sense/depth)
    API Endpoint: [/depth](https://api.mbodi.ai/sense/depth)
    """
    verbose = ctx.obj['VERBOSE']
    dry_run = ctx.obj['DRY_RUN']

    if list:
        click.echo("Available Depth Estimation Models:")
        click.echo("- Depth Anything")
        click.echo("- Zoe Depth")
        ctx.exit()

    if image_filename is None:
        click.echo("Error: Missing argument 'IMAGE_FILENAME'. Specify an image filename")
        return
    
    if verbose:
        click.echo(f"Running depth estimation on {image_filename}")

    if dry_run:
        click.echo(f"Dry run: Would estimate from image in {image_filename}")
        return

    image = Image(image_filename, size=(224, 224))
    agent = DepthEstimationAgent(model_src=model_src)
    result = agent.act(image=image, api_name=api_name)
    result.pil.show()

@sense.command("segment")
@click.argument("image_filename", required=False)
@click.option("--model-src", default="https://api.mbodi.ai/sense/", help="The model source URL.")
@click.option(
    "--segment-type",
    type=click.Choice(["bbox", "coords"], case_sensitive=False),
    prompt=False,
    help="Type of input data `bbox` (bounding box) or `coords` (pixel coordinates).",
)
@click.option(
    "--segment-input",
    prompt=False,
    help="Bounding box coordinates as x1,y1,x2,y2 or pixel coordinates as u,v.",
)
@click.option("--api-name", default="/segment", help="The API endpoint to use.")
@click.option("--list", "-l", is_flag=True, help="List available models for segmentation.")
@click.pass_context
def segment(ctx, image_filename, model_src, segment_type, segment_input, api_name, list) -> None:
    """Run the SegmentationAgent to segment objects in an image.

    Example command:
        mbodied sense segment resources/color_image.png --segment-type "bbox" --segment-input "50,50,150,150"

    Response:
        Masks shape:
        (1, 720, 1280)

    Inputs:
        [image_filename]: Path to the image file.
        [segment-type]: The type of segmentation input, either `bbox` for bounding box or `coords` for pixel coordinates.
        [segment-input]: The input data, either bounding box coordinates as `x1,y1,x2,y2` or pixel coordinates as `u,v`.

    Outputs:
        [Masks]: A 2D mask indicating the segmented region in the image.

    Loaded as API: [https://api.mbodi.ai/sense/segment](https://api.mbodi.ai/sense/segment)
    API Endpoint: [/segment](https://api.mbodi.ai/sense/depth)
    """
    verbose = ctx.obj['VERBOSE']
    dry_run = ctx.obj['DRY_RUN']

    if list:
        click.echo("Available Segmentation Models:")
        click.echo("- Segment Anything(SAM2)")
        ctx.exit()

    if image_filename is None:
        click.echo("Error: Missing argument 'IMAGE_FILENAME'. Specify an image filename")
        return
    
    if segment_type is None:
        segment_type = click.prompt(
            "Input type - bounding box or pixel coordinates", 
            type=click.Choice(["bbox", "coords"], case_sensitive=False)
        )

    if segment_input is None:
        segment_input = click.prompt(
            "Segment input data - x1,y1,x2,y2 (for bbox) or u,v (for coords)"
        )

    if verbose:
        click.echo(f"Running segmentation agent on {image_filename} to segment {segment_input}")

    if dry_run:
        click.echo(f"Dry run: Would segment objects in {image_filename}")
        return

    image = Image(image_filename, size=(224, 224))
    agent = SegmentationAgent(model_src=model_src)

    if segment_type == "bbox":
        bbox_coords = list(map(int, segment_input.split(",")))
        input_data = BBox2D(x1=bbox_coords[0], y1=bbox_coords[1], x2=bbox_coords[2], y2=bbox_coords[3])
    elif segment_type == "coords":
        u, v = map(int, segment_input.split(","))
        input_data = PixelCoords(u=u, v=v)

    mask_image, masks = agent.act(image=image, input_data=input_data, api_name=api_name)
    print("Masks shape:", masks.shape)
    mask_image.pil.show()

@cli.group(invoke_without_command=True)
@click.option("--list", "-l", is_flag=True, help="List available models for motion.")
@click.pass_context
def motion(ctx, list):
    """Commands related to robot motion tasks."""
    if list:
        click.echo("Available Motion Models:")
        click.echo("- OPENVLA MODEL")
        ctx.exit()
    
    if ctx.invoked_subcommand is None:
            click.echo("No subcommand provided. Try 'mbodied motion --help' for help")

@motion.command("openvla")
@click.argument("image_filename")
@click.option("--instruction", prompt="Instruction", help="Instruction for the OpenVlaAgent.")
@click.option("--model-src", default="https://api.mbodi.ai/community-models/", help="The model source URL.")
@click.option("--unnorm-key", default="bridge_orig", help="Key for the unnormalized image.")
@click.pass_context
def openvla_motion(ctx, instruction, image_filename, model_src, unnorm_key) -> None:
    """Run the OpenVlaAgent to generate robot motion based on instruction and image.
    
    Example command:
        mbodied motion openvla resources/xarm.jpeg --instruction "move forward"

    Response:
        Motion Response:
        HandControl(
            pose=Pose6D(
                x=-0.000432461563,
                y=0.000223397129,
                z=-0.000241243806,
                roll=-0.000138880808,
                pitch=0.00122899628,
                yaw=-6.67113405e-05
            ),
            grasp=JointControl(value=0.996078431)
        )

    Inputs:
        [image_filename]: Path to the image file.
        [instruction]: Instruction for the robot to act on.

    Outputs:
        [Motion Response]: HandControl object containing pose and grasp information.
    Loaded as API: [https://api.mbodi.ai/community-models/](https://api.mbodi.ai/community-models/)
    """
    verbose = ctx.obj['VERBOSE']
    dry_run = ctx.obj['DRY_RUN']

    if verbose:
        click.echo(f"Running OpenVLA motion agent on {image_filename} with instruction: {instruction}")

    if dry_run:
        click.echo(f"Dry run: Would generate robot motion from {image_filename} with instruction: {instruction}")
        return

    image = Image(image_filename, size=(224, 224))
    agent = OpenVlaAgent(model_src=model_src)
    motion_response = agent.act(instruction=instruction, image=image, unnorm_key=unnorm_key)
    
    print("Motion Response:", motion_response.flatten())

@cli.command("auto")
@click.argument("task")
@click.option("--image-path", default=None, help="Optional path to the image file (for sense tasks).")
@click.option("--model-src", default="openai", help="Model source for agent")
@click.option("--api-key", default=None, help="API key for the remote model, if applicable.")
@click.option("--params", type=str, help="JSON string with parameters for the agent.")
@click.pass_context
def auto(ctx, task, image_path, model_src, api_key, params):
    """Dynamically select and run the correct agent based on the task.

    Example command:
        mbodied auto language --params "{\"instruction\": \"Tell me a math joke?\"}"

    Response:
        Why was the equal sign so humble?
        Because it knew it wasn't less than or greater than anyone else!

    Example command:
        mbodied auto motion-openvla --params "{\"instruction\": \"Move forward\", \"image\": \"resources/bridge_example.jpeg\"}" --model-src "https://api.mbodi.ai/community-models/"

    Response:
        Response: HandControl(pose={'x': -0.00960310545, 'y': -0.0111081966, 'z': -0.00206002074, 'roll': 0.0126330038, 'pitch': -0.000780597846, 'yaw': -0.0177964902}, grasp={'value': 0.996078431})

    Inputs:
        [task]: Task to be executed by the agent. Choices include:
            - language: Run language-related tasks.
            - motion-openvla: Use the OpenVlaAgent to generate robot motion.
            - sense-object-detection: Run object detection tasks.
            - sense-image-segmentation: Run image segmentation tasks.
            - sense-depth-estimation: Run depth estimation tasks.

        [image-path]: (Optional) Path to an image file, required for sense and motion tasks.
        [model-src]: The source of the model, e.g., "openai", "gradio", etc.
        [api-key]: (Optional) API key for accessing the remote model.
        [params]: The parameters for the agent.

    Outputs:
        [Response]: The output generated by the selected agent based on the task, such as HandControl for motion or detected objects for sensing tasks.
    """
    verbose = ctx.obj['VERBOSE']
    dry_run = ctx.obj['DRY_RUN']

    if verbose:
        click.echo(f"Executing 'auto' command with task: {task}")

    if dry_run:
        click.echo(f"Dry run: Would execute 'auto' with task: {task}")
        return

    if params:
        try:
            options = json.loads(params)
        except json.JSONDecodeError:
            click.echo("Invalid JSON format for parameters.")
            return
    else:
        options = {}
    if "image" not in options:
        image = Image(image_path) if image_path else None
        options["image"] = image
    else:
        options["image"] = Image(options["image"])
    model_kwargs = {"api_key": api_key} if api_key else {}
    kwargs = options
    auto_agent = AutoAgent(task=task, model_src=model_src, model_kwargs=model_kwargs)

    response = auto_agent.act(**kwargs)
    if verbose:
        click.echo(f"[Verbose] Auto agent response: {response}")
    print(f"Response: {response}")

@cli.command("version")
def version():
    """Display the version of mbodied."""
    click.echo(f"mbodied version: {__version__}")

if __name__ == "__main__":
    cli()
