import os
import torch
import torch.nn as nn
import torch.nn.functional as F
import pytorch_lightning as pl
from timm import create_model
from x_transformers import TransformerWrapper, Decoder
from PIL import Image
from torch.utils.data import DataLoader, Dataset
from torchvision import transforms
from timm.loss import asymmetric_loss
from transformers import GPT2Tokenizer
from classifier_free_guidance_pytorch import (
    TextConditioner,
    AttentionTextConditioner,
    classifier_free_guidance,
)

NUM_ACTIONS = 7          # Number of robotic actions (x, y, z, roll, pitch, yaw, grasp)
ACTION_BINS = 255       # Discretization bins per action
EMBEDDING_DIM = 384     # Embedding dimension of ViT-Small
NUM_DECODER_LAYERS = 6
NUM_DECODER_HEADS = 8


class CustomDataset(Dataset):
    def __init__(self, video_folder, instructions, transform=None):
        self.video_folder = video_folder
        self.instructions = instructions
        self.video_files = os.listdir(video_folder)
        self.transform = transform

    def __len__(self):
        return len(self.video_files)

    def __getitem__(self, idx):
        video_name = os.path.join(self.video_folder, self.video_files[idx])
        video_frames = self.load_video(video_name)  # Load video frames

        if self.transform:
            video_frames = self.transform(video_frames)

        instruction = self.instructions[idx]  # Get corresponding instruction
        return video_frames, instruction

    def load_video(self, video_path):
        # Dummy implementation to load video frames
        return torch.randn(10, 3, 224, 224)  # 10 frames of 224x224 RGB images

transform = transforms.Compose([
    transforms.Resize((224, 224)),
    transforms.ToTensor(),
    transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5]),
])


class FilmLayer(nn.Module):
    def __init__(self, dim):
        super().__init__()
        self.gamma = nn.Parameter(torch.ones(dim))
        self.beta = nn.Parameter(torch.zeros(dim))

    def forward(self, x):
        return F.relu(x * self.gamma + self.beta)


class CustomDecoder(nn.Module):
    def __init__(self, num_tokens, dim, depth, heads, max_seq_len):
        super().__init__()
        self.transformer = TransformerWrapper(
            num_tokens=num_tokens,
            max_seq_len=max_seq_len,
            attn_layers=Decoder(dim=dim, depth=depth, heads=heads, cross_attend=True)
        )

        # Create a separate FilmLayer for each decoder layer
        self.film_layers = nn.ModuleList([FilmLayer(dim) for _ in range(depth)])

    def forward(self, x, context, condition_fns):
        """Forward pass of the decoder.

        Args:
            x (torch.Tensor): Input sequence of shape (batch_size, seq_len, dim).
            context (torch.Tensor): Contextual information from the encoder (e.g., image features).
            condition_fns: The conditioning functions generated by the TextConditioner or AttentionTextConditioner for each layer in the ViT
        """

        # Apply transformer layers interleaved with Film layers
        for i, layer in enumerate(self.transformer.attn_layers.layers):
            x = layer(x, context=context)
            x = self.film_layers[i](x, condition_fns[i])  # Apply the corresponding Film layer
        return x


# class Residual(nn.Module):
#     """Implements a residual connection in a transformer block.

#     This allows the gradient to flow more easily through deep networks by adding the input to the output.
#     """
#     def __init__(self, fn: nn.Module):
#         super().__init__()
#         self.fn = fn

#     def forward(self, x, *args, **kwargs):
#         return self.fn(x, *args, **kwargs) + x


# class FilmLayer(nn.Module):
#     def __init__(self, dim):
#         super().__init__()
#         self.gamma = nn.Parameter(torch.ones(dim))
#         self.beta = nn.Parameter(torch.zeros(dim))

#     def forward(self, x, condition_fn):
#         gamma = condition_fn(self.gamma)
#         beta = condition_fn(self.beta)
#         return x * gamma + beta


# class CustomDecoder(nn.Module):
#     def __init__(self, num_tokens, dim, depth, heads, max_seq_len):
#         super().__init__()
#         self.transformer = TransformerWrapper(
#             num_tokens=num_tokens,
#             max_seq_len=max_seq_len,
#             attn_layers=Decoder(dim=dim, depth=depth, heads=heads, cross_attend=True)
#         )

#         # Create a separate FilmLayer for each decoder layer
#         self.film_layers = nn.ModuleList([FilmLayer(dim) for _ in range(depth)])

#         # Initialize FilmTextConditioner for the decoder
#         self.film = FilmTextConditioner(hidden_dims=[dim] * depth, cond_drop_prob=0.2)

#     def forward(self, x, context, instructions):
#         # Generate per-layer conditioning functions from instructions
#         condition_fns = self.film(instructions)

#         # Apply transformer layers interleaved with Film layers
#         for i, layer in enumerate(self.transformer.attn_layers.layers):
#             x = Residual(layer)(x, context=context)  
#             x = self.film_layers[i](x, condition_fns[i])  
#         return x

# class CustomDecoder(nn.Module):
#     def __init__(self, num_tokens, dim, depth, heads, max_seq_len):
#         super().__init__()
#         assert len(dim) == depth, "length of dim must be equal to depth"
#         self.layers = nn.ModuleList([])
#         for i in range(depth):
#             should_cross_attend = i == 0
#             self.layers.append(nn.ModuleList([
#                 Residual(Attention(dim, heads = heads, cross_attend=should_cross_attend)),
#                 Residual(FeedForward(dim)),
#                 FilmLayer(dim),
#             ]))

#     def forward(self, x, context, condition_fns):
#         """Forward pass of the decoder.

#         Args:
#             x (torch.Tensor): Input sequence of shape (batch_size, seq_len, dim).
#             context (torch.Tensor): Contextual information from the encoder (e.g., image features).
#             condition_fns: The conditioning functions generated by the TextConditioner or AttentionTextConditioner for each layer in the ViT
#         """

#         # Apply transformer layers interleaved with Film layers
#         for i, (attn, ff, film_layer) in enumerate(self.layers):
#             x = attn(x, context=context) 
#             x = ff(x)
#             x = film_layer(x, condition_fns[i])

#         return x


# Tokenize Actions (x, y, z, roll, pitch, yaw, grasp)
ACTION_TOKENS = {
    'x': 0,
    'y': 1,
    'z': 2,
    'roll': 3,
    'pitch': 4,
    'yaw': 5,
    'grasp': 6
}

# Token Learner if needed
class TokenLearner(nn.Module):
    def __init__(self, dim, num_output_tokens):
        super().__init__()
        self.token_learner = nn.Sequential(
            nn.Linear(dim, num_output_tokens),
            nn.GELU(),
            nn.Linear(num_output_tokens, num_output_tokens)
        )

    def forward(self, x):
        return self.token_learner(x)


# Define the pattern transformation and integrate into training loop
class RT1(pl.LightningModule):
    def __init__(self, encoder, decoder, lr=1e-4, num_actions=7, action_bins=255, use_attn_conditioner=False):
        super().__init__()
        self.encoder = encoder
        self.decoder = decoder
        self.loss_fn = asymmetric_loss() 
        self.learning_rate = lr
        self.tokenizer = GPT2Tokenizer.from_pretrained("gpt2")
        self.action_embedding = nn.Embedding(self.tokenizer.vocab_size, self.encoder.embed_dim)
        self.num_actions = num_actions
        self.action_bins = action_bins
        self.logits = None
        # Add to_logits layer
        self.to_logits = nn.Sequential(
            nn.LayerNorm(encoder.embed_dim),  # Normalize the decoder's output
            nn.Linear(encoder.embed_dim, num_actions * action_bins),
            nn.ReLU()
            # Rearrange("... (a b) -> ... a b", b=action_bins), #Reshape logits into the desired format
        )


        # Determine which conditioner to use
        film_layer = AttentionTextConditioner if use_attn_conditioner else TextConditioner

        # Create the film conditioner
        self.conditioner = film_layer(
            hidden_dims=tuple(encoder.embed_dim for _ in range(len(encoder.blocks))),
            hiddens_channel_first=True,
            cond_drop_prob=0.2,
        )


    def forward(self, video, instructions, cond_scale=1.0, test=False):
        # Video should have shape (batch_size, num_frames, channels, height, width)
        # Instructions should be a list of strings (one per video in the batch)
        batch_size = video.shape[0]
        num_frames = video.shape[2]

        # Preprocessing (add your preprocessing steps here)
        video = self.preprocess(video) 

        # Flatten the video for the encoder (DINOv2)
        encoded_video = self.encoder(video.flatten(0, 1))  

        # Tokenize instructions
        encoded_instructions = self.tokenizer(
            instructions, padding="longest", return_tensors="pt"
        ).to(self.device)  # Ensure instructions are on the same device as the model

        # Generate per-layer conditioning functions from instructions
        condition_fns = self.conditioner(encoded_instructions["input_ids"])

        # Project actions to same embedding space as encoder
        action_embeddings = self.action_embedding(encoded_instructions["input_ids"])

        # Apply film layers to each block of the encoder
        conditioned_features = []
        for i, block in enumerate(self.encoder.blocks):
            for layer in block.layers:
                encoded_video[:, i] = condition_fns[i](encoded_video[:, i])
            conditioned_features.append(encoded_video[:, i]) # Store conditioned features for later use

        # Token learner (if needed)
        # ... (I will add token learner here if required)

        # Cross-attention decoder
        seq = torch.cat(
            (
                action_embeddings,  # Use action embeddings
                conditioned_features[-1],  # Last layer's features
            ),
            dim=1,
        )
        seq = self.decoder(seq, context=encoded_video, condition_fns=condition_fns) 

        # Classification logits
        logits = self.to_logits(seq)
        logits = logits.view(-1, num_frames, self.num_actions, self.action_bins)

        if test:
            # Apply classifier-free guidance during inference
            logits = classifier_free_guidance(self, video, texts=instructions, cond_scale=cond_scale)

        return logits

    def train(self, video, instructions):
        raise NotImplementedError

# Load pretrained ViT-g/14 model from timm and setup the model
def load_vit_model():
    vit_model = create_model('vit_small_patch14_reg4_dinov2', pretrained=True, features_only=True)
    return vit_model

# Example usage
if __name__ == '__main__':
    # Data transforms
    transform = transforms.Compose([
        transforms.Resize((224, 224)),
        transforms.ToTensor(),
        transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5]),
    ])

    # Dummy instructions
    # instructions = ["move to position (1, 2, 3) with roll 10, pitch 20, yaw 30 and grasp", "another instruction"]

    # Dataset and DataLoader
    # dataset = CustomDataset(video_folder='path_to_your_videos', instructions=instructions, transform=transform)
    # dataloader = DataLoader(dataset, batch_size=2, shuffle=True)

    # Model setup
    vit_model = load_vit_model()
    decoder = CustomDecoder(num_tokens=11, dim=vit_model.embed_dim, depth=6, heads=8, max_seq_len=7)
    model = RT1(encoder=vit_model, decoder=decoder)

    # Create a random video tensor
    batch_size = 2    # Simulating a batch of 2 videos
    num_frames = 10
    channels = 3      # Assuming RGB images
    height = 224
    width = 224

    video = torch.randn(batch_size, channels, num_frames, height, width)

    instructions = [
    "Pick up the blue cube and place it on the red square.",
    "Move forward until you see a green triangle, then stop."
    ]

    logits = model(video, instructions)  # Training mode (no CFG)

    # For inference, you can set test=True
    logits_inference = model(video, instructions, test=True, cond_scale=3.0)
